{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db08ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CarsPCL\n",
    "from model import PointNetCls\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15af3de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'Datasets'\n",
    "blue = lambda x: '\\033[94m' + x + '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1eea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_back = CarsPCL(path, 'back')\n",
    "dataset_front = CarsPCL(path, 'front')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca63c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will have 10 samples per batch\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54465f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load back dataset\n",
    "train_dataset_back, test_dataset_back = random_split(dataset_back, [int(len(dataset_back)*0.8), int(len(dataset_back)*0.2)])\n",
    "\n",
    "train_loader_back = DataLoader(train_dataset_back, batch_size=batch_size, shuffle=True, num_workers=1)  # tmp\n",
    "test_loader_back = DataLoader(test_dataset_back, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b26121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load front dataset\n",
    "train_dataset_front, test_dataset_front = random_split(dataset_front, [int(len(dataset_front)*0.8), int(len(dataset_front)*0.2)])\n",
    "\n",
    "train_loader_front = DataLoader(train_dataset_front, batch_size=batch_size, shuffle=True, num_workers=4)  # tmp\n",
    "test_loader_front = DataLoader(test_dataset_front, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faac7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_mix = DataLoader(test_dataset_front+test_dataset_back, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd1e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batch = len(train_dataset_back)/batch_size -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb58adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train (model, loader_train, loader_test, optimizer, scheduler, epochs):\n",
    "    for epoch in range(epochs):\n",
    "            scheduler.step()\n",
    "            for i, data in enumerate(loader_train, 0):  #Train against back\n",
    "                points, target = data\n",
    "                target = target[:, 0]\n",
    "                points = points.transpose(2, 1)\n",
    "                # points, target = points.cuda(), target.cuda() commented out as I do not have CUDA\n",
    "                optimizer.zero_grad()\n",
    "                model = model.train()\n",
    "                pred, trans, trans_feat = model(points)\n",
    "                loss = F.nll_loss(pred, target)\n",
    "                if feature_transform:\n",
    "                    loss += feature_transform_regularizer(trans_feat) * 0.001\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                pred_choice = pred.data.max(1)[1]\n",
    "                correct = pred_choice.eq(target.data).cpu().sum()\n",
    "                #print('[%d: %d/%d] train loss: %f accuracy: %f' % (epoch, i, num_batch, loss.item(), correct.item() / float(batch_size)))\n",
    "                if i % 10 == 0:\n",
    "                    j, data = next(enumerate(loader_test, 0))  # Test against FRONT\n",
    "                    points, target = data\n",
    "                    target = target[:, 0]\n",
    "                    points = points.transpose(2, 1)\n",
    "                    # points, target = points.cuda(), target.cuda() commented out as I do not have CUDA\n",
    "                    model = model.eval()\n",
    "                    pred, _, _ = model(points)\n",
    "                    loss = F.nll_loss(pred, target)\n",
    "                    pred_choice = pred.data.max(1)[1]\n",
    "                    correct = pred_choice.eq(target.data).cpu().sum()\n",
    "                    #print('[%d: %d/%d] %s loss: %f accuracy: %f' % (epoch, i, num_batch, blue('test'), loss.item(), correct.item()/float(batch_size)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed295d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_only(model, loader_test):\n",
    "    total_correct = 0\n",
    "    total_testset = 0\n",
    "    for i,data in tqdm(enumerate(loader_test, 0)):\n",
    "        points, target = data\n",
    "        target = target[:, 0]\n",
    "        points = points.transpose(2, 1)\n",
    "        # points, target = points.cuda(), target.cuda() commented out as I do not have CUDA\n",
    "        model = model.eval()\n",
    "        pred, _, _ = model(points)\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "        correct = pred_choice.eq(target.data).cpu().sum()\n",
    "        total_correct += correct.item()\n",
    "        total_testset += points.size()[0]\n",
    "    return total_correct/float(total_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e6fef",
   "metadata": {},
   "source": [
    "### Connect with OMNeT++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d70fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zmq\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "272d28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define back model, its optimizer and scheduler\n",
    "feature_transform = False\n",
    "classifier_bk_a = PointNetCls(k = len(dataset_back.classes), feature_transform=feature_transform)\n",
    "optimizer_bk_a = optim.Adam(classifier_bk_a.parameters(), lr=1e-5, betas=(0.9, 0.999))\n",
    "scheduler_bk_a = optim.lr_scheduler.StepLR(optimizer_bk_a, step_size=20, gamma=0.5)\n",
    "\n",
    "classifier_bk_b = PointNetCls(k = len(dataset_back.classes), feature_transform=feature_transform)\n",
    "optimizer_bk_b = optim.Adam(classifier_bk_b.parameters(), lr=1e-5, betas=(0.9, 0.999))\n",
    "scheduler_bk_b = optim.lr_scheduler.StepLR(optimizer_bk_b, step_size=20, gamma=0.5)\n",
    "\n",
    "classifier_bk_c = PointNetCls(k = len(dataset_back.classes), feature_transform=feature_transform)\n",
    "optimizer_bk_c = optim.Adam(classifier_bk_c.parameters(), lr=1e-5, betas=(0.9, 0.999))\n",
    "scheduler_bk_c = optim.lr_scheduler.StepLR(optimizer_bk_c, step_size=20, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14d121b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define front model, its optimizer and scheduler\n",
    "feature_transform = False\n",
    "classifier_ft_a = PointNetCls(k = len(dataset_back.classes), feature_transform=feature_transform)\n",
    "optimizer_ft_a = optim.Adam(classifier_ft_a.parameters(), lr=1e-5, betas=(0.9, 0.999))\n",
    "scheduler_ft_a = optim.lr_scheduler.StepLR(optimizer_ft_a, step_size=20, gamma=0.5)\n",
    "\n",
    "classifier_ft_b = PointNetCls(k = len(dataset_back.classes), feature_transform=feature_transform)\n",
    "optimizer_ft_b = optim.Adam(classifier_ft_b.parameters(), lr=1e-5, betas=(0.9, 0.999))\n",
    "scheduler_ft_b = optim.lr_scheduler.StepLR(optimizer_ft_b, step_size=20, gamma=0.5)\n",
    "\n",
    "classifier_ft_c = PointNetCls(k = len(dataset_back.classes), feature_transform=feature_transform)\n",
    "optimizer_ft_c = optim.Adam(classifier_ft_c.parameters(), lr=1e-5, betas=(0.9, 0.999))\n",
    "scheduler_ft_c = optim.lr_scheduler.StepLR(optimizer_ft_c, step_size=20, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73b4815c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train back a\n",
      "Train back b\n",
      "Train back c\n",
      "Train front a\n",
      "Train front b\n",
      "Train front c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:01, 14.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before merging back a  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 15.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before merging back b  0.04666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "15it [00:01, 13.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before merging back c  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "15it [00:01, 14.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before merging front a  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "15it [00:01, 13.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before merging front b  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 15.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before merging front c  0.0\n",
      "Epoch 1\n",
      "Sending to OMNeT++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after merging back a  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "15it [00:01, 13.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after merging back b  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "15it [00:01, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after merging back c  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "15it [00:01, 14.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after merging front a  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "15it [00:00, 15.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after merging front b  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "15it [00:00, 15.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after merging front c  0.0\n",
      "Train back a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7955/2646590174.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Train back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train back a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mclassifier_bk_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_bk_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_back\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_bk_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_bk_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# bk_a to back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train back b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mclassifier_bk_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_bk_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_back\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader_mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_bk_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_bk_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7955/1458997195.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader_train, loader_test, optimizer, scheduler, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfeature_transform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/omnetpp-5.6/workspace/FL/simulations/FL/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/omnetpp-5.6/workspace/FL/simulations/FL/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mn_pts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mtrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/omnetpp-5.6/workspace/FL/simulations/FL/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2280\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2282\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2283\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2284\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "acc_back, acc_front = 0, 0\n",
    "acc_bk_a_list, acc_bk_b_list, acc_bk_c_list = [], [], []\n",
    "acc_ft_a_list, acc_ft_b_list, acc_ft_c_list = [], [], []\n",
    "\n",
    "acc_bk_a_after_list, acc_bk_b_after_list, acc_bk_c_after_list = [], [], []\n",
    "acc_ft_a_after_list, acc_ft_b_after_list, acc_ft_c_after_list = [], [], []\n",
    "ii=0\n",
    "while True:\n",
    "    # Train back\n",
    "    print(\"Train back a\")\n",
    "    classifier_bk_a = train(classifier_bk_a, train_loader_back, test_loader_mix, optimizer_bk_a, scheduler_bk_a, 1)  # bk_a to back\n",
    "    print(\"Train back b\")\n",
    "    classifier_bk_b = train(classifier_bk_b, train_loader_back, test_loader_mix, optimizer_bk_b, scheduler_bk_b, 1)\n",
    "    print(\"Train back c\")\n",
    "    classifier_bk_c = train(classifier_bk_c, train_loader_back, test_loader_mix, optimizer_bk_c, scheduler_bk_c, 1)\n",
    "    # Train front\n",
    "    print(\"Train front a\")\n",
    "    classifier_ft_a = train(classifier_ft_a, train_loader_front, test_loader_mix, optimizer_ft_a, scheduler_ft_a, 1) # bk_a to front\n",
    "    print(\"Train front b\")\n",
    "    classifier_ft_b = train(classifier_ft_b, train_loader_front, test_loader_mix, optimizer_ft_b, scheduler_ft_b, 1)\n",
    "    print(\"Train front c\")\n",
    "    classifier_ft_c = train(classifier_ft_c, train_loader_front, test_loader_mix, optimizer_ft_c, scheduler_ft_c, 1)\n",
    "    \n",
    "    avgd = [(classifier_bk_a.state_dict()[key]+classifier_ft_a.state_dict()[key] + \n",
    "            classifier_bk_b.state_dict()[key]+classifier_ft_b.state_dict()[key] + \n",
    "            classifier_bk_c.state_dict()[key]+classifier_ft_c.state_dict()[key])/6 for key in classifier_bk_a.state_dict().keys()]\n",
    "    \n",
    "    \n",
    "    # Nearest neighbor scenario\n",
    "#     avgd_bk_a_ft_a = [(classifier_bk_a.state_dict()[key]+classifier_ft_a.state_dict()[key])/2 for key in classifier_bk_a.state_dict().keys()]\n",
    "#     avgd_ft_a_bk_b = [(classifier_ft_a.state_dict()[key]+classifier_bk_b.state_dict()[key])/2 for key in classifier_bk_a.state_dict().keys()]\n",
    "#     avgd_bk_b_ft_b = [(classifier_bk_b.state_dict()[key]+classifier_ft_b.state_dict()[key])/2 for key in classifier_bk_a.state_dict().keys()]\n",
    "#     avgd_ft_b_bk_c = [(classifier_ft_b.state_dict()[key]+classifier_bk_c.state_dict()[key])/2 for key in classifier_bk_a.state_dict().keys()]\n",
    "#     avgd_bk_c_ft_c = [(classifier_bk_c.state_dict()[key]+classifier_ft_c.state_dict()[key])/2 for key in classifier_bk_a.state_dict().keys()]\n",
    "#     avgd_ft_c_bk_a = [(classifier_ft_c.state_dict()[key]+classifier_bk_a.state_dict()[key])/2 for key in classifier_bk_a.state_dict().keys()]\n",
    "    \n",
    "    # Save some percent of parameters, back\n",
    "    pars_avgd_list_bk_a = []\n",
    "    for i, key in enumerate(classifier_bk_a.state_dict().keys()):\n",
    "        pars_avgd_list_bk_a.append((key, avgd[i])) if i>len(classifier_bk_a.state_dict())-np.ceil(len(classifier_bk_a.state_dict())*1) else pars_avgd_list_bk_a.append((key, classifier_bk_a.state_dict()[key]))     \n",
    "    pars_avgd_list_bk_b = []\n",
    "    for i, key in enumerate(classifier_bk_b.state_dict().keys()):\n",
    "        pars_avgd_list_bk_b.append((key, avgd[i])) if i>len(classifier_bk_b.state_dict())-np.ceil(len(classifier_bk_b.state_dict())*1) else pars_avgd_list_bk_b.append((key, classifier_bk_b.state_dict()[key]))   \n",
    "    pars_avgd_list_bk_c = []\n",
    "    for i, key in enumerate(classifier_bk_c.state_dict().keys()):\n",
    "        pars_avgd_list_bk_c.append((key, avgd[i])) if i>len(classifier_bk_c.state_dict())-np.ceil(len(classifier_bk_c.state_dict())*1) else pars_avgd_list_bk_c.append((key, classifier_bk_c.state_dict()[key]))\n",
    "    # Save some percent of parameters, front\n",
    "    pars_avgd_list_ft_a = []\n",
    "    for i, key in enumerate(classifier_ft_a.state_dict().keys()):\n",
    "        pars_avgd_list_ft_a.append((key, avgd[i])) if i>len(classifier_ft_a.state_dict())-np.ceil(len(classifier_ft_a.state_dict())*1) else pars_avgd_list_ft_a.append((key, classifier_ft_a.state_dict()[key]))      \n",
    "    pars_avgd_list_ft_b = []\n",
    "    for i, key in enumerate(classifier_ft_b.state_dict().keys()):\n",
    "        pars_avgd_list_ft_b.append((key, avgd[i])) if i>len(classifier_ft_b.state_dict())-np.ceil(len(classifier_ft_b.state_dict())*1) else pars_avgd_list_ft_b.append((key, classifier_ft_b.state_dict()[key])) \n",
    "    pars_avgd_list_ft_c = []\n",
    "    for i, key in enumerate(classifier_ft_c.state_dict().keys()):\n",
    "        pars_avgd_list_ft_c.append((key, avgd[i])) if i>len(classifier_ft_c.state_dict())-np.ceil(len(classifier_ft_c.state_dict())*1) else pars_avgd_list_ft_c.append((key, classifier_ft_c.state_dict()[key]))\n",
    "    \n",
    "    # Test back\n",
    "    acc_bk_a = test_only(classifier_bk_a, test_loader_mix) \n",
    "    print(\"Accuracy before merging back a \", acc_bk_a)\n",
    "    acc_bk_a_list.append(acc_bk_a)\n",
    "    acc_bk_b = test_only(classifier_bk_b, test_loader_mix)\n",
    "    print(\"Accuracy before merging back b \", acc_bk_b)\n",
    "    acc_bk_b_list.append(acc_bk_b)\n",
    "    acc_bk_c = test_only(classifier_bk_c, test_loader_mix)\n",
    "    print(\"Accuracy before merging back c \", acc_bk_c)\n",
    "    acc_bk_c_list.append(acc_bk_c)\n",
    "    # Test front\n",
    "    acc_ft_a = test_only(classifier_ft_a, test_loader_mix)\n",
    "    print(\"Accuracy before merging front a \", acc_ft_a)\n",
    "    acc_ft_a_list.append(acc_ft_a)\n",
    "    acc_ft_b = test_only(classifier_ft_b, test_loader_mix)\n",
    "    print(\"Accuracy before merging front b \", acc_ft_b)\n",
    "    acc_ft_b_list.append(acc_ft_b)\n",
    "    acc_ft_c = test_only(classifier_ft_c, test_loader_mix)\n",
    "    print(\"Accuracy before merging front c \", acc_ft_c)\n",
    "    acc_ft_c_list.append(acc_ft_c)\n",
    "\n",
    "    \n",
    "    ii+=1\n",
    "    print(f\"Epoch {ii}\")\n",
    "    if ii == 13:\n",
    "        break\n",
    "    # Connect to OMNeT ++\n",
    "    print(\"Sending to OMNeT++\")\n",
    "    context = zmq.Context()\n",
    "    socket = context.socket(zmq.REQ)\n",
    "    port = \"5555\"\n",
    "    socket.connect(\"tcp://localhost:%s\" % port)\n",
    "\n",
    "    message_from_python = ['0'.encode(), str(classifier_bk_a.state_dict()).encode(),\n",
    "                           '1'.encode(), str(classifier_ft_a.state_dict()).encode(), \n",
    "                           '2'.encode(), str(classifier_bk_b.state_dict()).encode(),\n",
    "                           '3'.encode(), str(classifier_ft_b.state_dict()).encode(),\n",
    "                           '4'.encode(), str(classifier_bk_c.state_dict()).encode(),\n",
    "                           '5'.encode(), str(classifier_ft_c.state_dict()).encode(), ]\n",
    "\n",
    "    socket.send_multipart(message_from_python)\n",
    "    message = socket.recv_multipart()\n",
    "    \n",
    "    # Save parameters back\n",
    "    pars_avgd_bk_a = OrderedDict(pars_avgd_list_bk_a)\n",
    "    pars_avgd_bk_b = OrderedDict(pars_avgd_list_bk_b)\n",
    "    pars_avgd_bk_c = OrderedDict(pars_avgd_list_bk_c)\n",
    "    # Save parameters front\n",
    "    pars_avgd_ft_a = OrderedDict(pars_avgd_list_ft_a)\n",
    "    pars_avgd_ft_b = OrderedDict(pars_avgd_list_ft_b)\n",
    "    pars_avgd_ft_c = OrderedDict(pars_avgd_list_ft_c)\n",
    "    # Load parameters back\n",
    "    classifier_bk_a.load_state_dict(pars_avgd_bk_a)\n",
    "    classifier_bk_b.load_state_dict(pars_avgd_bk_b)\n",
    "    classifier_bk_c.load_state_dict(pars_avgd_bk_c)\n",
    "    # Load parameters front\n",
    "    classifier_ft_a.load_state_dict(pars_avgd_ft_a)\n",
    "    classifier_ft_b.load_state_dict(pars_avgd_ft_b)\n",
    "    classifier_ft_c.load_state_dict(pars_avgd_ft_c)\n",
    "    \n",
    "    # Test new parameters back\n",
    "    acc_bk_a_after = test_only(classifier_bk_a, test_loader_mix)\n",
    "    print(\"Accuracy after merging back a \", acc_bk_a_after)\n",
    "    acc_bk_a_after_list.append(acc_bk_a_after)\n",
    "    acc_bk_b_after = test_only(classifier_bk_b, test_loader_mix)\n",
    "    print(\"Accuracy after merging back b \", acc_bk_b_after)\n",
    "    acc_bk_b_after_list.append(acc_bk_b_after)\n",
    "    acc_bk_c_after = test_only(classifier_bk_c, test_loader_mix)\n",
    "    print(\"Accuracy after merging back c \", acc_bk_c_after)\n",
    "    acc_bk_c_after_list.append(acc_bk_c_after)\n",
    "    # Test new parameters front\n",
    "    acc_ft_a_after = test_only(classifier_ft_a, test_loader_mix)\n",
    "    print(\"Accuracy after merging front a \", acc_ft_a_after)\n",
    "    acc_ft_a_after_list.append(acc_ft_a_after)\n",
    "    acc_ft_b_after = test_only(classifier_ft_b, test_loader_mix)\n",
    "    print(\"Accuracy after merging front b \", acc_ft_b_after)\n",
    "    acc_ft_b_after_list.append(acc_ft_b_after)\n",
    "    acc_ft_c_after = test_only(classifier_ft_c, test_loader_mix)\n",
    "    print(\"Accuracy after merging front c \", acc_ft_c_after)\n",
    "    acc_ft_c_after_list.append(acc_ft_c_after)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92890641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5def603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
